# -*- coding: utf-8 -*-
"""ML Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kwVql4xQu7JY3883-KnIv3V9700qUdec

# **1. Importing needed packages and explain their uses in your code.**
"""

# 1. Importing needed packages and explain their uses in your code.
import numpy as np  # for numerical operations
#Pandas offers data structure and operations for powerful, flexible, and easy-to-use data analysis and manipulation
import pandas as pd  # for data manipulation -
import matplotlib.pyplot as plt  # for data visualization
from sklearn.model_selection import train_test_split  # for data splitting
from sklearn.preprocessing import MinMaxScaler, StandardScaler  # for feature scaling
from sklearn.svm import SVC  # for Support Vector Classifier
from sklearn.metrics import accuracy_score,confusion_matrix, classification_report  # for evaluation
import seaborn as sns # Seaborn is a library for making statistical graphics in Python.
# Scikit-learn's DecisionTreeClassifier to build the decision tree model
from sklearn.tree import DecisionTreeClassifier
# Import the plot_tree function from sklearn.tree for visualizing decision tree models
from sklearn.tree import plot_tree
# Estimate Knn model and report the outcome :
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline

"""# **2. Importing the selected dataset and Explore & visualizing the dataset contents.**"""

# Load the dataset
# import Dataset
data = pd.read_csv('/content/sample_data/student-mat.csv', delimiter=';')
data.head(5)

# count if there are null value in cloulm
data.isna().sum()

# Display a summary of the DataFrame, including the data types of each column,
# the number of non-null values, and memory usage
data.info() # display dataset information

# Visualize age distribution
plt.figure(figsize=(8, 6))
sns.histplot(data['age'], bins=20, kde=True, color='skyblue')
plt.title('Distribution of Age')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.show()

sns.countplot(x='G3', hue='sex', data=data)
plt.show()

# sns.pairplot(data)
import seaborn as sns
import matplotlib.pyplot as plt

selected_columns = ['G1', 'G2', 'G3', 'studytime', 'failures', 'absences']
sns.pairplot(data[selected_columns])

"""<font color="green">Plot pairwise relationships in a dataset.

By default, this function will create a grid of Axes such that each numeric variable in data will by shared across the y-axes across a single row and the x-axes across a single column. The diagonal plots are treated differently: a univariate distribution plot is drawn to show the marginal distribution of the data in each column.</font>

# **Data Preprocessing**

**<font color="red">Encoding Categorical Variables</font>**
---
"""

#One Hot Encoding Categorical Variables Training Data
#get all categorical columns
cat_columns = data.select_dtypes(['object']).columns

#convert all categorical columns to numeric
data[cat_columns] = data[cat_columns].apply(lambda x: pd.factorize(x)[0])

#print head of data after convert
data.head(7)

data.info()

"""# **Convert G3 to classification categories**"""

# Convert G3 to classification categories
# 0 = Fail (0–9), 1 = Pass (10–14), 2 = Excellent (15–20)
data['G3_cat'] = pd.cut(data['G3'], bins=[-1, 9, 14, 20], labels=[0, 1, 2]).astype(int)
data.drop(columns=['G3'], inplace=True)

data.head()

"""# **Separate features (X) from the target variable (Y)**

"""

# Separate features and target
X_data = data.drop('G3_cat', axis=1)
y_data = data['G3_cat']


X_data.head(5)

""">**<font color=green>Optional Steps:</font>**

**Applying Normalization to the Adult Income Dataset**

---

"""

from sklearn.preprocessing import MinMaxScaler, StandardScaler

# Apply Min-Max Normalization to features
scaler = MinMaxScaler()
data_normalized = scaler.fit_transform(X_data)

# Convert back to DataFrame for better readability
X_data = pd.DataFrame(data_normalized, columns=X_data.columns)

X_data.head()

"""# **3. Splitting the dataset into train, test dataset & Feature Scaling**"""

# Split data into train+val and test sets (80% train+val, 20% test)
X_train_val, X_test, y_train_val, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42, stratify=y_data)

# Split train+val into actual train and validation (60% train, 20% val, 20% test overall)
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42, stratify=y_train_val)

X_train_val

"""# **4. Setting up the selected SVM ML model.**"""

# Optional: Use entire training+validation set for GridSearchCV
# GridSearchCV will internally perform its own cross-validation
X_grid = X_train_val
y_grid = y_train_val

# Step 2: Define a pipeline with preprocessing and model
# StandardScaler normalizes features before training SVM (very important for SVM)
pipeline = Pipeline([
    ('scaler', StandardScaler()),  # Feature scaling
    ('svm', SVC())                 # Support Vector Classifier
])

# Define the hyperparameter grid to search
param_grid = {
    'svm__C': [0.1, 1, 10],                 # Regularization strength
    'svm__kernel': ['linear', 'rbf'],      # Kernel type
    'svm__gamma': ['scale', 'auto']        # Kernel coefficient (only for 'rbf')
}

# Step 3: Perform grid search with 5-fold cross-validation
# n_jobs=-1 uses all available CPU cores for faster computation
grid_search = GridSearchCV(
    pipeline,
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)

# Fit GridSearchCV on the combined training and validation data
grid_search.fit(X_grid, y_grid)

# Print the best hyperparameters and corresponding cross-validation accuracy
print("Best parameters:", grid_search.best_params_)
print("Best cross-validation accuracy:", grid_search.best_score_)

# Step 4: Evaluate the best model on the untouched test set
best_model = grid_search.best_estimator_  # Extract best pipeline
y_pred = best_model.predict(X_test)       # Predict on test data

# Print test accuracy and a detailed classification report
print("Test Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

"""**Testing and evaluating the trained SVM ML model(s) against the test dataset and report the result visually.**

# **5. Setting up the Decision tree classifier model**
Let's create a decision tree classifier model and train using Entropy as shown below:
"""

# perform training with entropy
# Decision tree with entropy
DTree_clf = DecisionTreeClassifier(criterion = "entropy", random_state = 42,max_depth = 3, min_samples_leaf = 5)
# Fit the model
DTree_clf.fit(X_train_val, y_train_val)

"""**Decision Tree Model Prediction and Performance Assessment**

"""

# Predictions on the test set
y_pred_DT = DTree_clf.predict(X_test)

# Confusion matrix and performance metrics
cm = confusion_matrix(y_test, y_pred_DT)
accuracy = accuracy_score(y_test, y_pred_DT)
report = classification_report(y_test, y_pred_DT)

print("Confusion Matrix:\n", cm)
print("Accuracy:", accuracy)
print("Classification Report:\n", report)

# Make predictions on the test data
y_pred_DT = DTree_clf.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred_DT)
print("Accuracy:", accuracy)

# Create a confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred_DT)

# Plot confusion matrix
plt.figure(figsize=(5,3))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Greens")
plt.title("Confusion Matrix")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

"""**Visual Representation of the Decision Tree Model**"""

# Plot the decision tree
plt.figure(figsize=(15, 7))  # Adjust the size as needed
plot_tree(DTree_clf, feature_names=X_train.columns,     class_names=['Fail', 'Pass', 'Excellent'], filled=True, rounded=True)

# Add title
plt.title('Decision Tree Visualization')

# Display the plot
plt.show()

"""# **6. Setting up the KNN classifier model**"""

knn = KNeighborsClassifier(n_neighbors=9)
knn.fit(X_train_val, y_train_val)

# Predictions on the test set
y_pred_knn = knn.predict(X_test)

# Confusion matrix and performance metrics
cm = confusion_matrix(y_test, y_pred_knn)
accuracy = accuracy_score(y_test, y_pred_knn)
report = classification_report(y_test, y_pred_knn)

print("Confusion Matrix:\n", cm)
print("Accuracy:", accuracy)
print("Classification Report:\n", report)

# Make predictions on the test data
y_pred_knn = knn.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred_knn)
print("Accuracy:", accuracy)

# Create a confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred_knn)

# Plot confusion matrix
plt.figure(figsize=(5, 3))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Reds")
plt.title("Confusion Matrix")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()